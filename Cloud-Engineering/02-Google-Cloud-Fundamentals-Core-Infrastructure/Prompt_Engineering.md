<h1>Prompt engineering</h1>

Google offers a generative AI model called gemini, acting as an always on collaborator. Gemini has access to a massive range of data including Google Cloud documentation, tutorials and samples. it can provide details suggestions and guide on resources, detailed gcloud commands and insert them into Cloud Shell.

<h2>What is generative AI?</h2>

**Generative AI** encompasses a broader range of models capable of generating various types of content beyond just text. It is a subset of AI that is capable of creating text, images or other data using generative models, of then in response to prompts. Models learn the patterns and structures from input training data and then create new data with similar characteristics.

**Prompt** a specific instruction, question or cue given to a computer program or user to initiate a specific action or response.

Generative AI uses include software development, healthcare, finance, entertainment, customer service and sales.

<h2>What is a large language model?</h2>

**Large Language Model**(LLM) specifically refers to a subset of generative AI models focusing on language tasks. They are large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes.

**Large**
* The size of the training datasets
* The number of parameters

**General purpose**
* Can solve common problems

**Pre-trained and then fine-tune**
* Pre-trained with a large dataset
* fine-tuned with a smaller dataset

<h3>How are LLMs trained?</h3>
When submitting a prompt to an LLM, it calculates the probability of the correct answer from it's pre-trained model

**Pre-training** includes giving a massive dataset of text, images and code to the model so it can learn the underlying structure and patterns of the language, helping it to understand and generate human language more effectively. the model is then able to select the most common correct response to the prompt.

**Hallucinations** are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. This occurs as LLMs only understand information that they are trained on, not aware of business proprietary or domain specific data or real time information. they often assume the prompt is true and does not ask for more context information or accuracy in it's trained data. Hallucinations can make the model more likely to generate incorrect or misleading information.
 
 **Challenges**
 * The model is not trained on enough data
 * The model is trained on noisy or dirty data
 * The model is not given enough context
 * The model is not given enough constraints


<h2>What is prompt engineering?</h2>

**Prompt** is the text you feed into the model

**Prompt engineering** is articulating your prompts to get the best response from the model.

<h3>Categories of Prompts</h3>

**Zero-shot prompts** contains no context or examples to assist the model

**One-shot prompts** provide one example to the model for context

**Few-shot prompt** provides at least two examples of the model for context

**Role Prompts** require a frame of reference for the model to work from as it answers the questions. Define what is required and in what context.

<h3>Elements of a prompt</h3>

<h4>Preamble</h4>

**Context** for the task

**Instruction/task** the task itself

**Example** (one-shot, few-shot)examples to guide the model

<h4>Input</h4>

**Input**(to predict to) the central request, what the instruction or task will be acted upon.


<h2>Prompt engineering best practices</h2>

**Write detailed and explicit instructions** be clear and concise in the prompts you feed the model.

**Define boundaries for the prompt** instruct the model on what to do. Give it fallback outputs that work in carious situations.

**Adopt a persona for your input** adding a persona for the model can provide meaningful context to help it focus on related questions, improving accuracy.

**Keep each sentence concise** long sentences can sometime produce suboptimal results. use shorter sentences and simpler tasks.